{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I develop the following strategy in order to predict toxicity of multilanguage comments:\n",
    "\n",
    "(1) 17368 comments from training set were selected and translated on six different languages using translators library;\n",
    "(2) for toxic and normal comments vocabularies were created separately and then combined;\n",
    "(3) the final model contain embedding layer, two layers with GRU cells and one Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Ignore useless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "input_dir = 'data/'\n",
    "def load_csv_data(input_dir, filename):\n",
    "    csv_path = input_dir + filename + \".csv\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "train_data_b01 = load_csv_data(input_dir, \"jigsaw-toxic-comment-train-processed-seqlen128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# constructing toxic labels for training dataset\n",
    "\n",
    "train_data_b01['total_toxicity'] = 0\n",
    "s = train_data_b01.shape[0]\n",
    "\n",
    "for i in range(s):\n",
    "    counter = 0\n",
    "    if train_data_b01[\"toxic\"][i] > 0:\n",
    "        counter += 1\n",
    "        train_data_b01['total_toxicity'][i] += train_data_b01[\"toxic\"][i]\n",
    "        \n",
    "    if train_data_b01[\"severe_toxic\"][i] > 0:\n",
    "        counter += 1\n",
    "        train_data_b01['total_toxicity'][i] += train_data_b01[\"severe_toxic\"][i]\n",
    "        \n",
    "    if train_data_b01[\"obscene\"][i] > 0:\n",
    "        counter += 1\n",
    "        train_data_b01['total_toxicity'][i] += train_data_b01[\"obscene\"][i]\n",
    "        \n",
    "    if train_data_b01[\"threat\"][i] > 0:\n",
    "        counter += 1\n",
    "        train_data_b01['total_toxicity'][i] += train_data_b01[\"threat\"][i]\n",
    "        \n",
    "    if train_data_b01[\"insult\"][i] > 0:\n",
    "        counter += 1\n",
    "        train_data_b01['total_toxicity'][i] += train_data_b01[\"insult\"][i]\n",
    "    \n",
    "    if train_data_b01[\"identity_hate\"][i] > 0:\n",
    "        counter += 1\n",
    "        train_data_b01['total_toxicity'][i] += train_data_b01[\"identity_hate\"][i]\n",
    "        \n",
    "    if counter > 0 :\n",
    "        train_data_b01['total_toxicity'][i] = train_data_b01['total_toxicity'][i]/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset\n",
    "\n",
    "data = { 'comment_text' : (train_data_b01['comment_text']),\n",
    "        'toxic' : train_data_b01['total_toxicity']   \n",
    "}\n",
    "\n",
    "train_data_mod = pd.DataFrame(data, columns = ['comment_text', 'toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating normal and toxic comments\n",
    "\n",
    "max_comment_size = 300\n",
    "\n",
    "# remove emoji from comments\n",
    "import emoji\n",
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text)\n",
    "\n",
    "comment_normal = []\n",
    "comment_toxic = []\n",
    "size = train_data_mod.shape[0]\n",
    "    \n",
    "for i in range(size):\n",
    "    comment = train_data_mod['comment_text'][i]\n",
    "    comment = give_emoji_free_text(comment)\n",
    "    comment = comment[0:max_comment_size].replace(\"<r\\s*/?>\", \" \").replace(\"[^a-zA-Z']\", \" \")\n",
    "    if train_data_mod['toxic'][i] == 0:\n",
    "        comment_normal.append(comment)\n",
    "    else:\n",
    "        comment_toxic.append(comment)\n",
    "        \n",
    "normar_labels = np.zeros(len(comment_normal))\n",
    "toxic_labels = np.ones(len(comment_toxic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 3, 9, 80, 2, 34, 18, 2, 2, 8, 7, 3, 2, 2, 2, 4, 2, 13, 5, 41, 2, 2, 3, 2, 3, 4, 13, 3, 10, 2, 2, 16, 2, 3, 2, 2, 2, 3, 16, 2, 4, 29, 2, 2, 2, 2, 6, 2, 2, 2, 37, 2, 3, 27, 2, 6, 2, 3, 3, 4, 3, 4, 2, 12, 2, 11, 2, 2, 5, 4, 2, 3, 2, 2, 2, 2, 16, 2, 6, 2, 3, 6, 5, 2, 2, 2, 3, 6, 2, 2, 2, 2, 3, 22, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 7, 13, 3, 10, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 9, 2, 2, 9, 4, 2, 2, 2, 2, 2, 2, 2, 5, 3, 2, 2, 5, 2, 2, 6, 2, 8, 4, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 4, 3, 2, 2, 2, 4, 4, 6, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 5, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 10, 24, 20, 3, 2, 2, 2, 4, 6, 6, 2, 4, 5, 2, 2, 2, 9, 2, 4, 2, 16, 3, 2, 13, 2, 2, 4, 2, 10, 2, 3, 2, 2, 3, 4, 5, 2, 2, 2, 2, 5, 3, 2, 2, 11, 5, 6, 3, 2, 4, 2, 2, 2, 4, 2, 2, 2, 3, 2, 4, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2]\n",
      "[2, 3, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# duplicates\n",
    "\n",
    "import collections\n",
    "print([count for item, count in collections.Counter(comment_normal).items() if count > 1])\n",
    "print([count for item, count in collections.Counter(comment_toxic).items() if count > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "\n",
    "comment_normal = list(dict.fromkeys(comment_normal))\n",
    "comment_toxic = list(dict.fromkeys(comment_toxic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print([count for item, count in collections.Counter(comment_normal).items() if count > 1])\n",
    "print([count for item, count in collections.Counter(comment_toxic).items() if count > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining comments into text containing several comments separated by separators\n",
    "\n",
    "def text_packet(comment_normal, comment_toxic, inds_n, inds_t, packet_size =25, separator = \"######\"):\n",
    "    normal_comments = []\n",
    "    toxic_comments = []\n",
    "    \n",
    "    for i in range(packet_size):\n",
    "        n_ind = inds_n.pop(i)\n",
    "        t_ind = inds_t.pop(i)\n",
    "        normal_comments.append(comment_normal[n_ind])\n",
    "        toxic_comments.append(comment_toxic[t_ind])\n",
    "        \n",
    "    text_normal = ''\n",
    "    text_toxic = ''\n",
    "    \n",
    "    for comment in normal_comments:\n",
    "        text_normal += comment + separator\n",
    "    \n",
    "    for comment in toxic_comments:\n",
    "        text_toxic += comment + separator\n",
    "        \n",
    "    return text_normal, text_toxic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back from text to list of comments\n",
    "\n",
    "def text_to_list(text, separator = \"######\"):\n",
    "    text_comments = []\n",
    "    text_comments = text.split(separator)\n",
    "    try:\n",
    "        text_comments.remove('')\n",
    "    except:\n",
    "        text_comments = text_comments\n",
    "    return text_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending prepared lists of comments, labels, langs\n",
    "def append_data_lists(text, toxic_list, lang_list, comment_list, toxic = 1, lang = 'tr'):\n",
    "    text_list = text_to_list(text, separator = \"######\")\n",
    "    n = len(text_list) \n",
    "    for i in range(n):\n",
    "        toxic_list.append(toxic)\n",
    "        lang_list.append(lang)\n",
    "        comment_list.append(text_list[i])\n",
    "    return toxic_list, lang_list, comment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation of random indexes for data selection\n",
    "def random_inds(size):\n",
    "    inds = list(np.random.permutation(size))\n",
    "    return inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to translate some train data into six different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating english comments\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "len_n = len(comment_normal)\n",
    "len_t = len(comment_toxic)\n",
    "\n",
    "inds_n_tr = random_inds(len_n)\n",
    "inds_t_tr = random_inds(len_t)\n",
    "\n",
    "inds_n_pt = random_inds(len_n)\n",
    "inds_t_pt = random_inds(len_t)\n",
    "\n",
    "inds_n_ru = random_inds(len_n)\n",
    "inds_t_ru = random_inds(len_t)\n",
    "\n",
    "inds_n_fr = random_inds(len_n)\n",
    "inds_t_fr = random_inds(len_t)\n",
    "\n",
    "inds_n_it = random_inds(len_n)\n",
    "inds_t_it = random_inds(len_t)\n",
    "\n",
    "inds_n_es = random_inds(len_n)\n",
    "inds_t_es = random_inds(len_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For translation we will use translators library and google API. Note, that the Google server block IP after some limit and we can continue process of translation only on next day. For this reason we will perform translation process in three iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install translators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import time\n",
    "    import translators as ts\n",
    "    toxic_list = []\n",
    "    lang_list = []\n",
    "    comment_list = []\n",
    "    API = ts.google\n",
    "    packet_size = 25\n",
    "    sleep_time = 2\n",
    "    num_iter = 24\n",
    "    \n",
    "    for iteration in range(num_iter):\n",
    "        import translators as ts    \n",
    "        print(\"Iteration: \" + str(iteration)) \n",
    "       \n",
    "        lang = 'tr'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_tr, inds_t_tr, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_tr = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_tr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_tr = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_tr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "    \n",
    "        lang = 'pt'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_pt, inds_t_pt, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_pt = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_pt, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_pt = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_pt, \n",
    "                                                            toxic_list, lang_list, comment_list,\n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "       \n",
    "        lang = 'ru'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_ru, inds_t_ru,\n",
    "                                    packet_size = packet_size)\n",
    "        text_t_ru = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_ru, \n",
    "                                                            toxic_list, lang_list, comment_list,                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_ru = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_ru, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "       \n",
    "    \n",
    "        lang = 'fr'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_fr, inds_t_fr, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_fr = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_fr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_fr = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_fr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "    \n",
    "        lang = 'it'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_it, inds_t_it, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_it = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_it, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_it = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_it, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "    \n",
    "        lang = 'es'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_es, inds_t_es, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_es = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_es, \n",
    "                                                            toxic_list, lang_list, \n",
    "                                                            comment_list, toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_es = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_es, \n",
    "                                                            toxic_list, lang_list, \n",
    "                                                            comment_list, toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "        \n",
    "except:\n",
    "    print(\"Error\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of translation avalible in the file mini_multilang_trainset_1.csv in the 'preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mini_multilang_trainset_1 = pd.DataFrame(columns = ['comment', 'toxic', 'lang'])\n",
    "mini_multilang_trainset_1['comment'] = comment_list\n",
    "mini_multilang_trainset_1['toxic'] = toxic_list\n",
    "mini_multilang_trainset_1['lang'] = lang_list\n",
    "\n",
    "mini_multilang_trainset_1.to_csv('mini_multilang_trainset_1.csv',index = False)\n",
    "\"\"\"\n",
    "preprocessed_dir = 'preprocessed_data/'\n",
    "\n",
    "mini_multilang_trainset_1 = load_csv_data(preprocessed_dir, \"mini_multilang_trainset_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it is neccessary to save unused indeces inds_n_tr, inds_t_tr, inds_n_pt, inds_t_pt, inds_n_ru, inds_t_ru, inds_n_fr, inds_t_fr, inds_n_it, inds_t_it, inds_n_es, inds_t_es. As it was mentioned earlier, the Google server block IP after some limit and we can continue process of translation only on next day.\n",
    "\n",
    "DataFrames with unused undeces are avalible in the files 'unused_normal_indeces_after_step_1.csv' and 'unused_toxic_indeces_after_step_1.csv' in the 'preprocessed_data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "unused_normal_indeces_1 = pd.DataFrame(columns = ['ind_tr', 'ind_pt', 'ind_ru', 'ind_fr', 'ind_it', 'ind_es'])\n",
    "unused_normal_indeces_1['ind_tr'] = inds_n_tr\n",
    "unused_normal_indeces_1['ind_pt'] = inds_n_pt\n",
    "unused_normal_indeces_1['ind_ru'] = inds_n_ru\n",
    "unused_normal_indeces_1['ind_fr'] = inds_n_fr\n",
    "unused_normal_indeces_1['ind_it'] = inds_n_it\n",
    "unused_normal_indeces_1['ind_es'] = inds_n_es\n",
    "\n",
    "unused_toxic_indeces_1 = pd.DataFrame(columns = ['ind_tr', 'ind_pt', 'ind_ru', 'ind_fr', 'ind_it', 'ind_es'])\n",
    "unused_toxic_indeces_1['ind_tr'] = inds_t_tr\n",
    "unused_toxic_indeces_1['ind_pt'] = inds_t_pt\n",
    "unused_toxic_indeces_1['ind_ru'] = inds_t_ru\n",
    "unused_toxic_indeces_1['ind_fr'] = inds_t_fr\n",
    "unused_toxic_indeces_1['ind_it'] = inds_t_it\n",
    "unused_toxic_indeces_1['ind_es'] = inds_t_es\n",
    "\n",
    "unused_normal_indeces_1.to_csv('unused_normal_indeces_after_step_1',index = False)\n",
    "unused_toxic_indeces_1.to_csv('unused_toxic_indeces_after_step_1.csv',index = False)\n",
    "\"\"\"\n",
    "\n",
    "unused_normal_indeces_after_step_1 = load_csv_data(preprocessed_dir, \"unused_normal_indeces_after_step_1\")\n",
    "\n",
    "unused_toxic_indeces_after_step_1 = load_csv_data(preprocessed_dir, \"unused_toxic_indeces_after_step_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare random indeces from step 1\n",
    "\n",
    "inds_n_tr = list(unused_normal_indeces_after_step_1['ind_tr'])\n",
    "inds_t_tr = list(unused_toxic_indeces_after_step_1['ind_tr'])\n",
    "\n",
    "inds_n_pt = list(unused_normal_indeces_after_step_1['ind_pt'])\n",
    "inds_t_pt = list(unused_toxic_indeces_after_step_1['ind_pt'])\n",
    "\n",
    "inds_n_ru = list(unused_normal_indeces_after_step_1['ind_ru'])\n",
    "inds_t_ru = list(unused_toxic_indeces_after_step_1['ind_ru'])\n",
    "\n",
    "inds_n_fr = list(unused_normal_indeces_after_step_1['ind_fr'])\n",
    "inds_t_fr = list(unused_toxic_indeces_after_step_1['ind_fr'])\n",
    "\n",
    "inds_n_it = list(unused_normal_indeces_after_step_1['ind_it'])\n",
    "inds_t_it = list(unused_toxic_indeces_after_step_1['ind_it'])\n",
    "\n",
    "inds_n_es = list(unused_normal_indeces_after_step_1['ind_es'])\n",
    "inds_t_es = list(unused_toxic_indeces_after_step_1['ind_es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import time\n",
    "    import translators as ts\n",
    "    toxic_list = []\n",
    "    lang_list = []\n",
    "    comment_list = []\n",
    "    API = ts.google\n",
    "    packet_size = 25\n",
    "    sleep_time = 2\n",
    "    num_iter = 11\n",
    "    \n",
    "    for iteration in range(num_iter):\n",
    "        import translators as ts    \n",
    "        print(\"Iteration: \" + str(iteration)) \n",
    "       \n",
    "        lang = 'tr'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_tr, inds_t_tr, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_tr = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_tr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_tr = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_tr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "    \n",
    "        lang = 'pt'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_pt, inds_t_pt, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_pt = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_pt, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_pt = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_pt, \n",
    "                                                            toxic_list, lang_list, comment_list,\n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "       \n",
    "        lang = 'ru'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_ru, inds_t_ru,\n",
    "                                    packet_size = packet_size)\n",
    "        text_t_ru = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_ru, \n",
    "                                                            toxic_list, lang_list, comment_list,                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_ru = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_ru, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "       \n",
    "    \n",
    "        lang = 'fr'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_fr, inds_t_fr, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_fr = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_fr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_fr = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_fr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "    \n",
    "        lang = 'it'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_it, inds_t_it, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_it = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_it, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_it = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_it, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "    \n",
    "        lang = 'es'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_es, inds_t_es, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_es = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_es, \n",
    "                                                            toxic_list, lang_list, \n",
    "                                                            comment_list, toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_es = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_es, \n",
    "                                                            toxic_list, lang_list, \n",
    "                                                            comment_list, toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "        \n",
    "except:\n",
    "    print(\"Error\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of translation avalible in the file 'mini_multilang_trainset_2.csv' in the 'preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mini_multilang_trainset_2 = pd.DataFrame(columns = ['comment', 'toxic', 'lang'])\n",
    "mini_multilang_trainset_2['comment'] = comment_list\n",
    "mini_multilang_trainset_2['toxic'] = toxic_list\n",
    "mini_multilang_trainset_2['lang'] = lang_list\n",
    "\n",
    "mini_multilang_trainset_2.to_csv('mini_multilang_trainset_2.csv',index = False)\n",
    "\"\"\"\n",
    "mini_multilang_trainset_2 = load_csv_data(preprocessed_dir, \"mini_multilang_trainset_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is neccessary to save unused indeces inds_n_tr, inds_t_tr, inds_n_pt, inds_t_pt, inds_n_ru, inds_t_ru, inds_n_fr, inds_t_fr, inds_n_it, inds_t_it, inds_n_es, inds_t_es. \n",
    "\n",
    "DataFrames with unused undeces are avalible in the files 'unused_normal_indeces_after_step_2.csv' and 'unused_toxic_indeces_after_step_2.csv' in the 'preprocessed_data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "unused_normal_indeces_2 = pd.DataFrame(columns = ['ind_tr', 'ind_pt', 'ind_ru', 'ind_fr', 'ind_it', 'ind_es'])\n",
    "unused_normal_indeces_2['ind_tr'] = inds_n_tr\n",
    "unused_normal_indeces_2['ind_pt'] = inds_n_pt\n",
    "unused_normal_indeces_2['ind_ru'] = inds_n_ru\n",
    "unused_normal_indeces_2['ind_fr'] = inds_n_fr\n",
    "unused_normal_indeces_2['ind_it'] = inds_n_it\n",
    "unused_normal_indeces_2['ind_es'] = inds_n_es\n",
    "\n",
    "unused_toxic_indeces_2 = pd.DataFrame(columns = ['ind_tr', 'ind_pt', 'ind_ru', 'ind_fr', 'ind_it', 'ind_es'])\n",
    "unused_toxic_indeces_2['ind_tr'] = inds_t_tr\n",
    "unused_toxic_indeces_2['ind_pt'] = inds_t_pt\n",
    "unused_toxic_indeces_2['ind_ru'] = inds_t_ru\n",
    "unused_toxic_indeces_2['ind_fr'] = inds_t_fr\n",
    "unused_toxic_indeces_2['ind_it'] = inds_t_it\n",
    "unused_toxic_indeces_2['ind_es'] = inds_t_es\n",
    "\n",
    "unused_normal_indeces_2.to_csv('unused_normal_indeces_2.csv',index = False)\n",
    "unused_toxic_indeces_2.to_csv('unused_toxic_indeces_2.csv',index = False)\n",
    "\"\"\"\n",
    "\n",
    "unused_normal_indeces_after_step_2 = load_csv_data(preprocessed_dir, \"unused_normal_indeces_after_step_2\")\n",
    "\n",
    "unused_toxic_indeces_after_step_2 = load_csv_data(preprocessed_dir, \"unused_toxic_indeces_after_step_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation of random indices from step 2\n",
    "\n",
    "inds_n_tr = list(unused_normal_indeces_after_step_2['ind_tr'])\n",
    "inds_t_tr = list(unused_toxic_indeces_after_step_2['ind_tr'])\n",
    "\n",
    "inds_n_pt = list(unused_normal_indeces_after_step_2['ind_pt'])\n",
    "inds_t_pt = list(unused_toxic_indeces_after_step_2['ind_pt'])\n",
    "\n",
    "inds_n_ru = list(unused_normal_indeces_after_step_2['ind_ru'])\n",
    "inds_t_ru = list(unused_toxic_indeces_after_step_2['ind_ru'])\n",
    "\n",
    "inds_n_fr = list(unused_normal_indeces_after_step_2['ind_fr'])\n",
    "inds_t_fr = list(unused_toxic_indeces_after_step_2['ind_fr'])\n",
    "\n",
    "inds_n_it = list(unused_normal_indeces_after_step_2['ind_it'])\n",
    "inds_t_it = list(unused_toxic_indeces_after_step_2['ind_it'])\n",
    "\n",
    "inds_n_es = list(unused_normal_indeces_after_step_2['ind_es'])\n",
    "inds_t_es = list(unused_toxic_indeces_after_step_2['ind_es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import time\n",
    "    import translators as ts\n",
    "    toxic_list = []\n",
    "    lang_list = []\n",
    "    comment_list = []\n",
    "    API = ts.google\n",
    "    packet_size = 25\n",
    "    sleep_time = 2\n",
    "    num_iter = 15\n",
    "    \n",
    "    for iteration in range(num_iter):\n",
    "        import translators as ts    \n",
    "        print(\"Iteration: \" + str(iteration)) \n",
    "       \n",
    "        lang = 'tr'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_tr, inds_t_tr, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_tr = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_tr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_tr = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_tr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "    \n",
    "        lang = 'pt'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_pt, inds_t_pt, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_pt = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_pt, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_pt = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_pt, \n",
    "                                                            toxic_list, lang_list, comment_list,\n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "       \n",
    "        lang = 'ru'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_ru, inds_t_ru,\n",
    "                                    packet_size = packet_size)\n",
    "        text_t_ru = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_ru, \n",
    "                                                            toxic_list, lang_list, comment_list,                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_ru = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_ru, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "       \n",
    "    \n",
    "        lang = 'fr'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_fr, inds_t_fr, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_fr = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_fr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_fr = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_fr, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "    \n",
    "        lang = 'it'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_it, inds_t_it, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_it = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_it, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_it = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_it, \n",
    "                                                            toxic_list, lang_list, comment_list, \n",
    "                                                            toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "    \n",
    "        lang = 'es'\n",
    "        text_n, text_t = text_packet(comment_normal, comment_toxic, inds_n_es, inds_t_es, \n",
    "                                     packet_size = packet_size)\n",
    "        text_t_es = API(text_t, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_t_es, \n",
    "                                                            toxic_list, lang_list, \n",
    "                                                            comment_list, toxic = 1, lang = lang)\n",
    "        time.sleep(sleep_time)\n",
    "        text_n_es = API(text_n, to_language=lang)\n",
    "        toxic_list, lang_list, comment_list = append_data_lists(text_n_es, \n",
    "                                                            toxic_list, lang_list, \n",
    "                                                            comment_list, toxic = 0, lang = lang)\n",
    "        time.sleep(sleep_time*2)\n",
    "        \n",
    "except:\n",
    "    print(\"Error\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of translation avalible in the file mini_multilang_trainset_3.csv in the 'preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mini_multilang_trainset_3 = pd.DataFrame(columns = ['comment', 'toxic', 'lang'])\n",
    "mini_multilang_trainset_3['comment'] = comment_list\n",
    "mini_multilang_trainset_3['toxic'] = toxic_list\n",
    "mini_multilang_trainset_3['lang'] = lang_list\n",
    "\n",
    "mini_multilang_trainset_3.to_csv('mini_multilang_trainset_3.csv',index = False)\n",
    "\"\"\"\n",
    "mini_multilang_trainset_3 = load_csv_data(preprocessed_dir, \"mini_multilang_trainset_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is neccessary to save unused indeces inds_n_tr, inds_t_tr, inds_n_pt, inds_t_pt, inds_n_ru, inds_t_ru, inds_n_fr, inds_t_fr, inds_n_it, inds_t_it, inds_n_es, inds_t_es.\n",
    "\n",
    "DataFrames with unused undeces are avalible in the files 'unused_normal_indeces_after_step_3.csv' and 'unused_toxic_indeces_after_step_3.csv' in the 'preprocessed_data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "unused_normal_indeces_3 = pd.DataFrame(columns = ['ind_tr', 'ind_pt', 'ind_ru', 'ind_fr', 'ind_it', 'ind_es'])\n",
    "unused_normal_indeces_3['ind_tr'] = inds_n_tr\n",
    "unused_normal_indeces_3['ind_pt'] = inds_n_pt\n",
    "unused_normal_indeces_3['ind_ru'] = inds_n_ru\n",
    "unused_normal_indeces_3['ind_fr'] = inds_n_fr\n",
    "unused_normal_indeces_3['ind_it'] = inds_n_it\n",
    "unused_normal_indeces_3['ind_es'] = inds_n_es\n",
    "\n",
    "unused_toxic_indeces_3 = pd.DataFrame(columns = ['ind_tr', 'ind_pt', 'ind_ru', 'ind_fr', 'ind_it', 'ind_es'])\n",
    "unused_toxic_indeces_3['ind_tr'] = inds_t_tr\n",
    "unused_toxic_indeces_3['ind_pt'] = inds_t_pt\n",
    "unused_toxic_indeces_3['ind_ru'] = inds_t_ru\n",
    "unused_toxic_indeces_3['ind_fr'] = inds_t_fr\n",
    "unused_toxic_indeces_3['ind_it'] = inds_t_it\n",
    "unused_toxic_indeces_3['ind_es'] = inds_t_es\n",
    "\n",
    "unused_normal_indeces_3.to_csv('unused_normal_indeces_3.csv',index = False)\n",
    "unused_toxic_indeces_3.to_csv('unused_toxic_indeces_3.csv',index = False)\n",
    "\"\"\"\n",
    "\n",
    "unused_normal_indeces_after_step_3 = load_csv_data(preprocessed_dir, \"unused_normal_indeces_after_step_3\")\n",
    "\n",
    "unused_toxic_indeces_after_step_3 = load_csv_data(preprocessed_dir, \"unused_toxic_indeces_after_step_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select 2800 english comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addition of english comments\n",
    "\n",
    "len_n = len(comment_normal)\n",
    "len_t = len(comment_toxic)\n",
    "\n",
    "num_eng_comments = 2800\n",
    "\n",
    "inds_n_en =random_inds(len_n)\n",
    "inds_t_en =random_inds(len_t)\n",
    "\n",
    "toxic_list_en = []\n",
    "lang_list_en = []\n",
    "comment_list_en = []\n",
    "\n",
    "for i in range(num_eng_comments//2):\n",
    "    ind = inds_n_en[i]\n",
    "    comment_list_en.append(comment_normal[ind])\n",
    "    lang_list_en.append('en')\n",
    "    toxic_list_en.append(0)\n",
    "    \n",
    "for i in range(num_eng_comments//2):\n",
    "    ind = inds_t_en[i]\n",
    "    comment_list_en.append(comment_toxic[ind])\n",
    "    lang_list_en.append('en')\n",
    "    toxic_list_en.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini english dataset avalible as 'mini_multilang_trainset_en_s2800.csv' in the directory 'preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mini_multilang_trainset_en = pd.DataFrame(columns = ['comment', 'toxic', 'lang'])\n",
    "mini_multilang_trainset_en['comment'] = comment_list_en\n",
    "mini_multilang_trainset_en['toxic'] = toxic_list_en\n",
    "mini_multilang_trainset_en['lang'] = lang_list_en\n",
    "\n",
    "mini_multilang_trainset_en.to_csv('mini_multilang_trainset_en_s2800.csv',index = False)\n",
    "\"\"\"\n",
    "mini_multilang_trainset_en = load_csv_data(preprocessed_dir, \"mini_multilang_trainset_en_s2800\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all mini train sets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Üzgünüm, ama adam bir pratt. Daha önce bana sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acıklı küçük kasaba fucktard. Ne kadar aptal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t\\nPHUQ OFF PEYNİR DÜĞMESİ!\\n\\nTUZ!\\nPHUQ OFF...</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...\\n\\nİNDİRİLDİĞİNİZDE NEDEN BU YOLDA ÇALIŞI...</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, görünüşünüz hiç yardımcı olmuyor. Tecavüz gi...</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic lang\n",
       "0  Üzgünüm, ama adam bir pratt. Daha önce bana sa...      1   tr\n",
       "1   Acıklı küçük kasaba fucktard. Ne kadar aptal ...      1   tr\n",
       "2   t\\nPHUQ OFF PEYNİR DÜĞMESİ!\\n\\nTUZ!\\nPHUQ OFF...      1   tr\n",
       "3   ...\\n\\nİNDİRİLDİĞİNİZDE NEDEN BU YOLDA ÇALIŞI...      1   tr\n",
       "4  , görünüşünüz hiç yardımcı olmuyor. Tecavüz gi...      1   tr"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [mini_multilang_trainset_1, mini_multilang_trainset_2, mini_multilang_trainset_3, mini_multilang_trainset_en]\n",
    "\n",
    "mini_multilang_trainset_combo  = pd.concat(frames)\n",
    "mini_multilang_trainset_combo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manyally check our dataset using, for example Exele or Numbers and delete some unique comments-artifacts  (I spend about 15 min checking comments). Corrected dataset avalible in the file 'mini_multilang_trainset_combo_manually_corrected.csv' in the directory 'preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Üzgünüm, ama adam bir pratt. Daha önce bana sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Acıklı küçük kasaba fucktard. Ne kadar aptal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>t\\nPHUQ OFF PEYNİR DÜĞMESİ!\\n\\nTUZ!\\nPHUQ OFF...</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>...\\n\\nİNDİRİLDİĞİNİZDE NEDEN BU YOLDA ÇALIŞI...</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>, görünüşünüz hiç yardımcı olmuyor. Tecavüz gi...</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            comment  toxic lang\n",
       "0           0  Üzgünüm, ama adam bir pratt. Daha önce bana sa...      1   tr\n",
       "1           1   Acıklı küçük kasaba fucktard. Ne kadar aptal ...      1   tr\n",
       "2           2   t\\nPHUQ OFF PEYNİR DÜĞMESİ!\\n\\nTUZ!\\nPHUQ OFF...      1   tr\n",
       "3           3   ...\\n\\nİNDİRİLDİĞİNİZDE NEDEN BU YOLDA ÇALIŞI...      1   tr\n",
       "4           4  , görünüşünüz hiç yardımcı olmuyor. Tecavüz gi...      1   tr"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_multilang_trainset_combo = load_csv_data(preprocessed_dir, \"mini_multilang_trainset_combo_manually_corrected\")\n",
    "\n",
    "mini_multilang_trainset_combo.drop_duplicates(keep=False,inplace=True)\n",
    "mini_multilang_trainset_combo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    2800\n",
       "ru    2491\n",
       "fr    2487\n",
       "it    2431\n",
       "tr    2425\n",
       "es    2389\n",
       "pt    2345\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_multilang_trainset_combo['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will change all characters to english using translit function, clean text from artifacts and trash and split normal and toxic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # clear data and time\n",
    "    text = re.sub('\\d{2}.\\d{2}.\\d{2}, \\d{2}:\\d{2}:\\d{2}', '', text)\n",
    "    text = re.sub('\\d{2}.\\d{2} \\d{2}:\\d{2}', '', text)\n",
    "    \n",
    "    # remove whitespace before and after word\n",
    "    text = re.sub('-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n|[«»]|[\"\"]|[><]|\"[\\[]]|//\"', '', text)\n",
    "    text = re.sub('[«»]|[\"\"]|[><]|\"[\\[]]\"', '', text)\n",
    "    text = re.sub('[~-¿:;_\"?*!@#$^&%()]|[+=]|[[]|[]]|[/]', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\r\\n\\t|\\n|\\r\\t|\\\\n|&gt', ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text)\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translit(string):\n",
    "    \"\"\" This function works just fine \"\"\"\n",
    "    capital_letters = {\n",
    "\n",
    "    }\n",
    "\n",
    "    lower_case_letters = {\n",
    "        u'а': u'a',\n",
    "        u'б': u'b',\n",
    "        u'в': u'v',\n",
    "        u'г': u'g',\n",
    "        u'д': u'd',\n",
    "        u'е': u'e',\n",
    "        u'ё': u'e',\n",
    "        u'ж': u'zh',\n",
    "        u'з': u'z',\n",
    "        u'и': u'i',\n",
    "        u'й': u'y',\n",
    "        u'к': u'k',\n",
    "        u'л': u'l',\n",
    "        u'м': u'm',\n",
    "        u'н': u'n',\n",
    "        u'о': u'o',\n",
    "        u'п': u'p',\n",
    "        u'р': u'r',\n",
    "        u'с': u's',\n",
    "        u'т': u't',\n",
    "        u'у': u'u',\n",
    "        u'ф': u'f',\n",
    "        u'х': u'h',\n",
    "        u'ц': u'ts',\n",
    "        u'ч': u'ch',\n",
    "        u'ш': u'sh',\n",
    "        u'щ': u'sch',\n",
    "        u'ъ': u'',\n",
    "        u'ы': u'y',\n",
    "        u'ь': u'',\n",
    "        u'э': u'e',\n",
    "        u'ю': u'yu',\n",
    "        u'я': u'ya',\n",
    "        \n",
    "        u'ö': u'o',\n",
    "        u'ü': u'u',\n",
    "        u'ş': u's',\n",
    "        u'ç': u'c',\n",
    "        u'ğ': u'g',\n",
    "        u'â': u'a',\n",
    "        u'i̇': u'i',\n",
    "        \n",
    "        u'ó': u'o',\n",
    "        u'é': u'e',\n",
    "        u'ñ': u'n',\n",
    "        u'á': u'a',\n",
    "        u'í': u'i',\n",
    "        \n",
    "        u'ã': u'a',\n",
    "        u'ú': u'u',\n",
    "        u'ê': u'e',\n",
    "        u'à': u'a',\n",
    "        u'õ': u'o',\n",
    "        u'ĩ': u'i',\n",
    "        u'è': u'i',\n",
    "    }\n",
    "\n",
    "    translit_string = \"\"\n",
    "\n",
    "    for index, char in enumerate(string):\n",
    "        if char in lower_case_letters.keys():\n",
    "            char = lower_case_letters[char]\n",
    "        elif char in capital_letters.keys():\n",
    "            char = capital_letters[char]\n",
    "            if len(string) > index+1:\n",
    "                if string[index+1] not in lower_case_letters.keys():\n",
    "                    char = char.upper()\n",
    "            else:\n",
    "                char = char.upper()\n",
    "        translit_string += char\n",
    "\n",
    "    return translit_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_to_lists(dataframe):\n",
    "    comment_normal = []\n",
    "    comment_toxic = []\n",
    "    size = dataframe.shape[0]\n",
    "\n",
    "    for i in range(size):\n",
    "        comment = dataframe['comment'][i]\n",
    "        lang = dataframe['lang'][i]\n",
    "        comment = clean_text(comment)\n",
    "        if lang != 'en':\n",
    "            comment = translit(comment)            \n",
    "        if dataframe['toxic'][i] == 0:\n",
    "            comment_normal.append(comment)\n",
    "        else:\n",
    "            comment_toxic.append(comment)\n",
    "        \n",
    "    normar_labels = np.zeros(len(comment_normal))\n",
    "    toxic_labels = np.ones(len(comment_toxic))\n",
    "    \n",
    "    return comment_normal, comment_toxic, normar_labels, toxic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: Possible nested set at position 27\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "comment_normal, comment_toxic, labels_normal, labels_toxic = separate_to_lists(mini_multilang_trainset_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text data\n",
    "\n",
    "def preprocess(X_batch, y_batch):\n",
    "    n_words = 128\n",
    "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.lower(X_batch)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    X_batch =X_batch.to_tensor(shape=shape, default_value=b\"<pad>\")\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tensorflow datasets\n",
    "\n",
    "dataset_normal =  tf.data.Dataset.from_tensor_slices((tf.constant(comment_normal, dtype=tf.string), \n",
    "                                                      tf.constant(labels_normal, dtype=tf.float32)))\n",
    "dataset_toxic =  tf.data.Dataset.from_tensor_slices((tf.constant(comment_toxic, dtype=tf.string), \n",
    "                                                     tf.constant(labels_toxic, dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "toxic_vocabulary = Counter()\n",
    "for X_batch, y_batch in dataset_toxic.batch(32).map(preprocess):\n",
    "    for comment in X_batch:\n",
    "        toxic_vocabulary.update(list(comment.numpy()))\n",
    "        \n",
    "normal_vocabulary = Counter()\n",
    "for X_batch, y_batch in dataset_normal.batch(32).map(preprocess):\n",
    "    for comment in X_batch:\n",
    "        normal_vocabulary.update(list(comment.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 880035),\n",
       " (b'de', 3930),\n",
       " (b'a', 3296),\n",
       " (b'que', 2953),\n",
       " (b'i', 2678),\n",
       " (b'e', 2370),\n",
       " (b'un', 1812),\n",
       " (b'you', 1763),\n",
       " (b'la', 1563),\n",
       " (b'ne', 1253)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_vocabulary.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 841741),\n",
       " (b'de', 5846),\n",
       " (b'a', 4213),\n",
       " (b'que', 3512),\n",
       " (b'i', 3051),\n",
       " (b'la', 2630),\n",
       " (b'the', 2310),\n",
       " (b'e', 2266),\n",
       " (b'o', 1643),\n",
       " (b'un', 1602)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_vocabulary.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39164, 48795)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic_vocabulary), len(normal_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will combine toxic and normal vacabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22143, 48795)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_toxic_vocabulary = toxic_vocabulary\n",
    "new_normal_vocabulary = normal_vocabulary\n",
    "\n",
    "toxic_vocabulary_list = list(new_toxic_vocabulary)\n",
    "normal_vocabulary_list = list(new_normal_vocabulary)\n",
    "\n",
    "for word in normal_vocabulary_list:\n",
    "    if new_toxic_vocabulary[word] != 0:\n",
    "        del new_toxic_vocabulary[word]\n",
    "\n",
    "len(new_toxic_vocabulary), len(new_normal_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'fuck', 550),\n",
       " (b'cunt', 148),\n",
       " (b'culo', 131),\n",
       " (b'foda', 113),\n",
       " (b'puta', 111),\n",
       " (b'suck', 92),\n",
       " (b'perra', 80),\n",
       " (b'joder', 79),\n",
       " (b'foutre', 72),\n",
       " (b'asshole', 72)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_toxic_vocabulary.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_vocab_size = 40000\n",
    "truncated_normal_vocabulary = [\n",
    "    word for word, count in new_normal_vocabulary.most_common()[:normal_vocab_size]]\n",
    "\n",
    "toxic_vocab_size = 20000\n",
    "truncated_toxic_vocabulary = [\n",
    "    word for word, count in new_toxic_vocabulary.most_common()[:toxic_vocab_size]]\n",
    "\n",
    "merged_vocabulary = truncated_normal_vocabulary + truncated_toxic_vocabulary\n",
    "len(merged_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "75\n",
      "19206\n",
      "4\n",
      "4173\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {word: index for index, word in enumerate(merged_vocabulary)}\n",
    "\n",
    "vocab_size = normal_vocab_size + toxic_vocab_size\n",
    "\n",
    "for word in b\"fuck this shit i hate it\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "18\n",
      "40084\n",
      "44598\n"
     ]
    }
   ],
   "source": [
    "for word in b\"pochel v pizda urod\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=int64, numpy=array([[40000,    75, 19206,     4,  4173,    50]])>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_oov_buckets = 5000\n",
    "\n",
    "words = tf.constant(merged_vocabulary)\n",
    "word_ids = tf.range(len(merged_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "table.lookup(tf.constant([b\"fuck this shit i hate it\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation of efficient tensorflow train set\n",
    "\n",
    "all_comments = comment_normal + comment_toxic\n",
    "all_labels = np.concatenate((np.zeros(len(comment_normal)), np.ones(len(comment_toxic))), axis = 0)\n",
    "\n",
    "train_dataset=  tf.data.Dataset.from_tensor_slices((tf.constant(all_comments, dtype=tf.string), \n",
    "                                                      tf.constant(all_labels, dtype=tf.float32)))\n",
    "\n",
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_set = train_dataset.repeat().shuffle(50000).batch(batch_size).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our model with simple GRU cells on this mini train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 135 steps\n",
      "Epoch 1/8\n",
      "135/135 [==============================] - 79s 583ms/step - loss: 0.4267 - accuracy: 0.7893\n",
      "Epoch 2/8\n",
      "135/135 [==============================] - 73s 540ms/step - loss: 0.1812 - accuracy: 0.9343\n",
      "Epoch 3/8\n",
      "135/135 [==============================] - 72s 530ms/step - loss: 0.0765 - accuracy: 0.9758\n",
      "Epoch 4/8\n",
      "135/135 [==============================] - 71s 529ms/step - loss: 0.0355 - accuracy: 0.9895\n",
      "Epoch 5/8\n",
      "135/135 [==============================] - 72s 535ms/step - loss: 0.0165 - accuracy: 0.9953\n",
      "Epoch 6/8\n",
      "135/135 [==============================] - 72s 535ms/step - loss: 0.0061 - accuracy: 0.9979\n",
      "Epoch 7/8\n",
      "135/135 [==============================] - 76s 565ms/step - loss: 0.0038 - accuracy: 0.9988\n",
      "Epoch 8/8\n",
      "135/135 [==============================] - 72s 535ms/step - loss: 0.0039 - accuracy: 0.9986\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_size = len(all_comments)\n",
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f69afc16450>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Losses\n",
    "plt.plot(np.arange(len(history.history[\"loss\"])) + 0.5, history.history[\"loss\"], \"b.-\", label=\"Training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6a042bc5d0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy\n",
    "plt.plot(np.arange(len(history.history[\"accuracy\"])) + 0.5, history.history[\"accuracy\"], \"b.-\", label=\"Training accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for training set in order to cherck confusion matrix\n",
    "pred_list = []\n",
    "label_list = []\n",
    "\n",
    "num_taken = all_labels.shape[0]//batch_size\n",
    "\n",
    "for (X_batch, y_batch) in train_set.take(num_taken):\n",
    "    batch_predictions = model.predict(X_batch)\n",
    "    \n",
    "    for prediction in batch_predictions:\n",
    "        pred_list.append(prediction)\n",
    "        \n",
    "    for label in y_batch:\n",
    "        label_list.append(label)\n",
    "        \n",
    "y_pred = np.asarray(pred_list).reshape((len(pred_list),))\n",
    "y_train = np.asarray(label_list).reshape((len(label_list),))\n",
    "y_train_pred = np.around(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9323,    5],\n",
       "       [   3, 7949]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.999537037037037\n",
      "precision_score: 0.999371385466432\n",
      "recall_score: 0.9996227364185111\n"
     ]
    }
   ],
   "source": [
    "# determination of accuracy, precision and recall scores for training set\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"accuracy_score: \" + str(accuracy_score(y_train, y_train_pred)))\n",
    "print(\"precision_score: \" + str(precision_score(y_train, y_train_pred)))\n",
    "print(\"recall_score: \" + str(recall_score(y_train, y_train_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will prepare the validation set and perform evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tr    3000\n",
       "it    2500\n",
       "es    2500\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparation of validation set\n",
    "\n",
    "valid_data_b = load_csv_data(input_dir,\"validation-processed-seqlen128\")\n",
    "\n",
    "valid_data_b['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_to_lists_2(dataframe):\n",
    "    comments = []\n",
    "    labels = []\n",
    "    size = dataframe.shape[0]\n",
    "\n",
    "    for i in range(size):\n",
    "        lang = dataframe['lang'][i]\n",
    "        label = dataframe['toxic'][i]\n",
    "        comment = dataframe['comment_text'][i]\n",
    "        comment = clean_text(comment)\n",
    "        if lang != 'en':\n",
    "            comment = translit(comment)                \n",
    "        comments.append(comment)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return comments, labels\n",
    "\n",
    "valid_comments, valid_labels = separate_to_lists_2(valid_data_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow dataset\n",
    "\n",
    "dataset_valid =  tf.data.Dataset.from_tensor_slices((tf.constant(valid_comments, dtype=tf.string), \n",
    "                                                      tf.constant(valid_labels, dtype=tf.float32)))\n",
    "\n",
    "valid_set = dataset_valid.repeat().batch(batch_size).map(preprocess)\n",
    "valid_set = valid_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 13s 204ms/step - loss: 1.4876 - accuracy: 0.7547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4876038278302839, 0.7546623]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_taken = len(valid_labels)//batch_size\n",
    "\n",
    "model.evaluate(valid_set.take(num_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions for validation set\n",
    "valid_pred_list = []\n",
    "valid_label_list = []\n",
    "\n",
    "num_taken = len(valid_labels)//batch_size\n",
    "\n",
    "for (X_batch, y_batch) in valid_set.take(num_taken):\n",
    "    batch_predictions = model.predict(X_batch)\n",
    "    \n",
    "    for prediction in batch_predictions:\n",
    "        valid_pred_list.append(prediction)\n",
    "        \n",
    "    for label in y_batch:\n",
    "        valid_label_list.append(label)\n",
    "        \n",
    "y_valid_prob = np.asarray(valid_pred_list).reshape((len(valid_pred_list),))\n",
    "y_valid = np.asarray(valid_label_list).reshape((len(valid_label_list),))\n",
    "y_valid_pred = np.around(y_valid_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5295, 1418],\n",
       "       [ 529,  694]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_valid_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.7546622983870968\n",
      "precision_score: 0.32859848484848486\n",
      "recall_score: 0.5674570727718724\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy_score: \" + str(accuracy_score(y_valid, y_valid_pred)))\n",
    "print(\"precision_score: \" + str(precision_score(y_valid, y_valid_pred)))\n",
    "print(\"recall_score: \" + str(recall_score(y_valid, y_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. 77% of accuracy is not bed, but precision and recall scores are so low. We will continue to train model on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 62 steps\n",
      "Epoch 1/5\n",
      "62/62 [==============================] - 33s 531ms/step - loss: 0.3555 - accuracy: 0.8652\n",
      "Epoch 2/5\n",
      "62/62 [==============================] - 34s 544ms/step - loss: 0.1476 - accuracy: 0.9447\n",
      "Epoch 3/5\n",
      "62/62 [==============================] - 35s 569ms/step - loss: 0.0360 - accuracy: 0.9903\n",
      "Epoch 4/5\n",
      "62/62 [==============================] - 33s 538ms/step - loss: 0.0073 - accuracy: 0.9984\n",
      "Epoch 5/5\n",
      "62/62 [==============================] - 33s 534ms/step - loss: 0.0013 - accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# continue training on validation data\n",
    "\n",
    "train_size = len(valid_labels)\n",
    "\n",
    "history = model.fit(valid_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6713,    0],\n",
       "       [   0, 1223]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions \n",
    "valid_pred_list = []\n",
    "valid_label_list = []\n",
    "\n",
    "num_taken = len(valid_labels)//batch_size\n",
    "\n",
    "for (X_batch, y_batch) in valid_set.take(num_taken):\n",
    "    batch_predictions = model.predict(X_batch)\n",
    "    \n",
    "    for prediction in batch_predictions:\n",
    "        valid_pred_list.append(prediction)\n",
    "        \n",
    "    for label in y_batch:\n",
    "        valid_label_list.append(label)\n",
    "        \n",
    "y_valid_prob = np.asarray(valid_pred_list).reshape((len(valid_pred_list),))\n",
    "y_valid = np.asarray(valid_label_list).reshape((len(valid_label_list),))\n",
    "y_valid_pred = np.around(y_valid_prob)\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_valid_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I suppose it was considerably improve situation. We can start working with test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>input_word_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>all_segment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
       "      <td>(101, 17376, 14516, 19165, 56324, 10116, 24542...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
       "      <td>(101, 511, 53204, 36689, 44504, 117, 11279, 57...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n",
       "      <td>(101, 35921, 17938, 13055, 13868, 11381, 10120...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
       "      <td>(101, 59170, 16822, 99087, 10284, 83972, 51782...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
       "      <td>(101, 131, 32070, 11759, 131, 11045, 23388, 10...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       comment_text  \\\n",
       "0   0  Doctor Who adlı viki başlığına 12. doctor olar...   \n",
       "1   1   Вполне возможно, но я пока не вижу необходимо...   \n",
       "2   2  Quindi tu sei uno di quelli   conservativi  , ...   \n",
       "3   3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   \n",
       "4   4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   \n",
       "\n",
       "                                      input_word_ids  \\\n",
       "0  (101, 17376, 14516, 19165, 56324, 10116, 24542...   \n",
       "1  (101, 511, 53204, 36689, 44504, 117, 11279, 57...   \n",
       "2  (101, 35921, 17938, 13055, 13868, 11381, 10120...   \n",
       "3  (101, 59170, 16822, 99087, 10284, 83972, 51782...   \n",
       "4  (101, 131, 32070, 11759, 131, 11045, 23388, 10...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                      all_segment_id  \n",
       "0  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_b = load_csv_data(input_dir,\"test-processed-seqlen128\")\n",
    "test_data_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63812, 5)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_to_lists_test_data(dataframe):\n",
    "    comments = []\n",
    "    size = dataframe.shape[0]\n",
    "\n",
    "    for i in range(size):\n",
    "        comment = dataframe['comment_text'][i]\n",
    "        comment = clean_text(comment)\n",
    "        comment = translit(comment)\n",
    "        comments.append(comment)\n",
    "    \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = separate_to_lists_test_data(test_data_b)\n",
    "pseudo_labels = np.zeros(len(test_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63812"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test =  tf.data.Dataset.from_tensor_slices((tf.constant(test_comments, dtype=tf.string), \n",
    "                                                      tf.constant(pseudo_labels, dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = dataset_test.batch(batch_size=1).map(preprocess)\n",
    "test_set = test_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63812"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction for test set\n",
    "test_pred_list = []\n",
    "for data in test_set.as_numpy_iterator():\n",
    "    X, y = data\n",
    "    prediction = model.predict(X)\n",
    "    test_pred_list.append(prediction)\n",
    "\n",
    "len(test_pred_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prob = np.asarray(test_pred_list).reshape((len(test_pred_list),))\n",
    "y_test_pred = y_test_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12051, 0.18885162665329405, 63812)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percent of toxic comments in the test set\n",
    "counter = 0\n",
    "\n",
    "for prediction in y_test_pred:\n",
    "    if prediction >= 0.5:\n",
    "        counter+=1\n",
    "        \n",
    "counter, counter/len(y_test_pred), len(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make submission file\n",
    "submission = load_csv_data(input_dir, \"sample_submission\")\n",
    "submission['toxic'] = y_test_pred\n",
    "submission.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
